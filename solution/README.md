## API для управления данными клиентов, рекламодателей, рекламными кампаниями, показом объявлений, статистикой и управлением "текущим днём" в системе

### Индивидуальный тур Заключительного этапа олимпиады PROD

<hr>


## Запуск проекта

Для успешного запуска необходимо наличие git, docker и docker-compose

```bash
git clone https://gitlab.prodcontest.ru/2025-final-projects-back/NikitaMulyar
cd solution/
docker compose up --build
```


### Устройство `docker-compose.yml` и `Dockerfile`

```dockerfile
services:
  app:
    build: .
    restart: always
    ports:
      - "8080:8080"
    env_file:
      - .env
    depends_on:
      - postgres
      - redis

  postgres:
    image: postgres:latest
    restart: always
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USERNAME}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DATABASE}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USERNAME} -d ${POSTGRES_DATABASE}" ]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:latest
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  redis_data:
```

```dockerfile
FROM python:3.12-alpine3.21

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "main:server_app", "--host", "REDACTED", "--port", "8080", "--reload"]

```

1. Сервисы используют образы `python3.12`, `postgres`, `redis` из Docker Hub, после чего происходит 
сборка с помощью Dockerfile.
2. Все необходимые зависимости хранятся в файле `requirements.txt`, которые автоматически начинают устанавливаться.
3. Загружаются переменные окружения из файла `.env`.
4. Сервер дожидается запуска `redis` и `postgres` с помощью healthcheck - это гарантирует, что запросы не начнут 
  обрабатываться раньше, чем нужно.
5. Отмечу, что контейнер настроен на автоматический перезапуск в случае сбоев.
6. После полной готовности контейнера запускается ASGI сервер, написанный на фреймворке FastAPI!
7. Вы шикарны и можете продавать слона!

<hr>


## Стек разработки

- Язык: `Python 3.12`. В мире разработки он занимает особое место, предоставляя разработчикам удобные и эффективные 
методы для создания API в силу большого числа фремворков. Этот язык выделяется своей простотой и читаемостью кода, 
что делает процесс разработки более интуитивно понятным и доступным для широкого круга разработчиков.


- СУБД: `PostgreSQL`. В отличие от SQLite, данная СУБД может быть масшатабируема на большие проекты. Также она 
  является лидером на российском рынке IT. СУБД проста в использовании, несмотря на, первый взгляд, трудности в 
  установке, засчет большому сообществу, которое развивает и поддерживает ее.


- Кеш-сервис: `Redis`. Данные хранятся в оперативной памяти сервера, что делает систему максимально производительной. 
Операции чтения и записи занимают меньше миллисекунды даже при большом потоке запросов. 


- ORM: `SQLAlchemy 2.0`. Тут можно по пунктам:
    - **Эффективность**. Позволяет взаимодействовать с базой данных, используя объекты и методы Python, без 
  необходимости писать отдельные SQL-запросы. Это ускоряет управление данными и их обработку.
    - **Независимость от базы данных**. Обеспечивает согласованный API, который абстрагирует различия между конкретными 
      базами данных. Это позволяет переключаться между разными системами баз данных (SQLite, PostgreSQL, MySQL и т. д.) 
  с минимальными изменениями в коде.


- Фреймворк: `FastAPI`. Фреймворк работает быстрее по сравнению с другими платформами Python. Также он позволяет 
  разработчикам писать более компактный код, пропуская дополнительные проверки. Поскольку фреймворк базируется на 
  библиотеке `Pydantic` (которая, кстати, тоже активно применяется в проекте), он обнаруживает неправильные типы 
  данных даже в глубоко вложенных запросах и возвращает обоснование в формате JSON.


- DevOps: `Docker compose`. Контейнеры намного легче и меньше, чем виртуальные машины. Они занимают меньше памяти и 
не требуют больших физических серверов.

### Дополнительная внешняя зависимость

- Модуль `g4f`. Данная библиотека предоставляет бесплатный доступ ко многим LLM. В проекте использована для 
  генерации текста рекламы.

<hr>


## Описание работы проекта

Поскольку проект написан на FastAPI, то, перейдя по ссылке `http://REDACTED:8080/docs`, фреймворк сформирует 
спецификацию по каждому эндпоинту. Это еще один плюс за использование именно этого фреймворка :)

Но все же, что можно делать?

#### Клиенты

- Создание и редактирование клиентов с возможностью персонализации (пол, локация, возраст)
- Получение клиентов по их уникальным идентификаторам

#### Рекламодатели

- Создание и редактирование рекламодателей
- Получение рекламодателей по их уникальным идентификаторам
- Создание связки "Рекламодатель - Клиент", которая обладает характеристикой ML-score (показатель релевантности)

#### Рекламные кампании

- Возможность тонкой настройки кампаний: даты начала и окончания, лимиты на показы и клики, таргетинг по полу, 
  диапазону возрастов и локации
- Возможность создавать, получать, обновлять и удалять созданные рекламодателем кампании
- В запросах на создание и обновление можно задать query-параметр `llm=1`! В таком случае текст рекламы будет 
  сгенерирован с помощью CharGPT o4-mini! (Обращю внимание, что, возможно, ответ придет с задержкой 10-20 секунд)
- Также есть возможность получать статистику: как для одной кампании, так и по всем кампаниям рекламодателя. Также 
  есть 2 вида: обычная (общая) и с разбивкой по дням, что так же важно в аналитике!

### Алгоритм показа рекламы клиентам

Чтобы выбрать рекламу, которую нужно показать пользователю, чтобы она была как наиболее релевантна для него, так и 
наиболее прибыльна для рекламодателя, был написан алгоритм.

#### Шаг 1. Фильтрация

Программа запрашивает список рекламных кампаний из базы данных, которые удовлетворяют условиям:

- кампания "идет" (текущая дата находится между датами начала и конца)
- локация кампании совпадает с локацией клиента
- возраст и пол клиента удовляетворяют ограничениям кампании

#### Шаг 2. Распределение

Далее начинается второй этап - полученные кампании делятся на 3 группы:

1. Пользователь может посмотреть и кликнуть (оба лимита не исчерпаны и эти кампании не были ранее просмотрены клиентом)
2. Пользователь может кликнуть по ранее просмотренной кампании (лимит на клики не исчерпан и эти кампании не были 
   ранее кликнуты клиентом)
3. В третьей группе те кампании, которые были кликнуты

Начиная с 1-й группы алгоритм ищет непустой список кампаний. Если все 3 группы пустые - эндпоинт вернет 404.

#### Шаг 3. Выборка

Найдя по иерархии первый непустой список кампаний, алгоритм начинает подсчет Combined Score для каждой кампании из 
этого списка.

Мы располагаем следующими метриками:

- ML-score
- CTR
- доход с перехода/клика

```python
# Нормализация ML-score (0-1)
normalized_ml = ml_score / max_ml_in_pool

# Нормализация прибыли (0-1) и сглаживание CTR
ctr_numerator = current_clicks + 1
ctr_denominator = current_impressions + 2
ctr = ctr_numerator / ctr_denominator
normalized_profit = (cost_per_impression + cost_per_click * ctr) / max_profit_in_pool

# Весовые коэффициенты (пример: α=0.8, β=0.2)
combined_score = α * normalized_profit + β * normalized_ml
```

Сортируем кампании по убыванию и берем первую. Не забываем в случае, если клиент не видел рекламу, увеличивать 
счетчик показов.

<hr>


## Структура проекта

- `/db`: модели SQLAlchemy для работы с СУБД
- `/redis`: функции для работы с Redis
- `/routers`: роутеры сервера, логично разделенные по файлам
- `/schemas`: Pydantic схемы данных запросов и ответов
- `/tests`: unit и e2e тесты

<hr>


## Схема базы данных

![Диаграмма](docs/db.png)

<hr>


## Автор

Муляр Никита, телеграм: https://t.me/delikatny_pon
